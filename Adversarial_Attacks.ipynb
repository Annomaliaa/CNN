{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annomaliaa/CNN/blob/master/Adversarial_Attacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-NZGKsavsfo5"
      },
      "outputs": [],
      "source": [
        "#import tensorflow as tf\n",
        "#from tensorflow.keras import Sequential\n",
        "#from tensorflow.keras.callbacks import LambdaCallback \n",
        "#import tensorflow.keras.layers as L\n",
        "#from tensorflow.keras.datasets import mnist, cifar10\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dill"
      ],
      "metadata": {
        "id": "Zj_r6HuHtdM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "526b69b9-010b-4756-fa4a-4a2e1d963653"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dill"
      ],
      "metadata": {
        "id": "EWZYUGBvtpFr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n"
      ],
      "metadata": {
        "id": "DFzl7_pMypu9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GZKX-3I4ytSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f824e29-9fe0-4e9b-f5df-09d1ea79d6d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath, pickle_module=dill, map_location=torch.device('cpu'))\n",
        "    model = checkpoint[\"model\"]\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"]) \n",
        "    return model\n"
      ],
      "metadata": {
        "id": "FnaDHj5a741H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_shapes(x_train, x_test, y_train, y_test):\n",
        "  print(f\"x_train: {x_train.shape}\\n\"\\\n",
        "      f\"x_test: {x_test.shape}\\n\"\\\n",
        "      f\"y_train: {y_train.shape}\\n\"\\\n",
        "      f\"y_test: {y_test.shape}\\n\")"
      ],
      "metadata": {
        "id": "-3NuVzoXx1CL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  transform_train = transforms.Compose([\n",
        "          transforms.RandomCrop(32, padding=4),\n",
        "          transforms.RandomHorizontalFlip(),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225]),\n",
        "      ])\n",
        "\n",
        "  transform_test = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225]),\n",
        "  ])"
      ],
      "metadata": {
        "id": "IIMnnmjcF8jD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = datasets.CIFAR10"
      ],
      "metadata": {
        "id": "ZuLFYaCVGVEM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = dataloader(root='./data', train=True, download=True, transform=transform_train)\n",
        "#trainset = (x_train, y_train)\n",
        "testset = dataloader(root='./data', train=False, download=False, transform=transform_test)\n",
        "#testset = (x_test, y_test)\n",
        "x_train = trainset.data\n",
        "y_train = trainset.targets\n",
        "x_test = testset.data\n",
        "y_test = testset.targets"
      ],
      "metadata": {
        "id": "O7qnk_q4xgYd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a107798-5e64-4b8a-a26e-1f7b1c20612b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:14<00:00, 11725421.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "BATCH_SIZE = 128\n",
        "trainloader = data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "testloader = data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "4ALS8nE2L05S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = 10\n",
        "label_names = ['airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog',\n",
        "               'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "id": "O_miBw3St2VV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing images and labels\n",
        "height, width, channels = 32, 32, 3\n",
        "nb_classes = 10 "
      ],
      "metadata": {
        "id": "TZjzbojExV4Z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert y_train to one-hot encoded format\n",
        "y_train = torch.nn.functional.one_hot(torch.tensor(y_train), num_classes=nb_classes).numpy()\n",
        "\n",
        "# Convert y_test to one-hot encoded format\n",
        "y_test = torch.nn.functional.one_hot(torch.tensor(y_test), num_classes=nb_classes).numpy()\n",
        "\n",
        "print_shapes(x_train, x_test, y_train, y_test)\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "x_train = x_train.reshape((-1, height, width, channels))\n",
        "x_test = x_test.reshape((-1, height, width, channels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTuDKl7vf0Ez",
        "outputId": "b55c658f-2a77-4472-a62f-0060b4795614"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train: (50000, 32, 32, 3)\n",
            "x_test: (10000, 32, 32, 3)\n",
            "y_train: (50000, 10)\n",
            "y_test: (10000, 10)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NET_NAME = \"resnet20-decomp-TT\"\n",
        "WEIGHTS_PATH = f'/content/drive/My Drive/CIFAR10/fine_tuned/{NET_NAME}.pth'\n",
        "model = load_checkpoint(WEIGHTS_PATH)"
      ],
      "metadata": {
        "id": "uFK0ot5f8Ar7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "# Load the image of frog or dog\n",
        "image_path = f'/content/drive/My Drive/CIFAR10/ATMC/emanuel-AKYjr-kmYtQ-unsplash.jpg'\n",
        "image = Image.open(image_path)\n",
        "\n",
        "print(image.size)\n",
        "\n",
        "# Check the number of channels in the image\n",
        "num_channels = len(image.getbands())\n",
        "\n",
        "transform = transforms.Compose([\n",
        "      transforms.Resize((32, 32)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225]),\n",
        "  ])\n",
        "\n",
        "\n",
        "# Apply the transformations\n",
        "\n",
        "input_image = transform(image).unsqueeze(0)\n",
        "image = input_image.to(device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    predictions = model(image)\n",
        "\n",
        "label = ['airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog',\n",
        "               'horse', 'ship', 'truck']\n",
        "\n",
        "\n",
        "print(predictions)\n",
        "predicted_class = torch.argmax(predictions).item()\n",
        "\n",
        "print(predicted_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXIoELCT2X0G",
        "outputId": "b683c230-0f0d-4eb1-e33a-1e8ba2f944c4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4068, 2199)\n",
            "tensor([[ 8.5302, -4.3004,  5.5711, -0.9800,  1.9730, -3.9284, -1.3962, -3.6276,\n",
            "         -0.1324, -1.7095]], device='cuda:0')\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Function to calculate adversary noise\n",
        "def generate_adversary(image, label):\n",
        "    device = torch.device('cpu') \n",
        "    image = torch.from_numpy(image)\n",
        "    image = image.to(torch.float32)\n",
        "    image = image.clone().detach().requires_grad_(True)\n",
        "\n",
        "    #Image shape expected input[1, 32, 32, 3] to have 3 channels, but got 32 channels \n",
        "    image = torch.transpose(image, 1, 3)\n",
        "    prediction = model(image)\n",
        "    #Prediction and label shapes need to match\n",
        "    print(\"Predictions\",prediction.shape)\n",
        "    print(\"Labels\", label.size)\n",
        "    label = torch.from_numpy(label)\n",
        "    label = label.to(torch.float32)\n",
        "\n",
        "    loss = F.mse_loss(prediction, label, size_average=None, reduce=None, reduction='mean')\n",
        "\n",
        "    print(\"Loss\", loss)\n",
        "\n",
        "    print(\"Predictions:\", prediction.dtype)\n",
        "    print(\"Label:\", label.dtype)\n",
        "    print(\"Loss:\", loss.dtype)\n",
        "    \n",
        "    print(\"Image:\", image.dtype)\n",
        "\n",
        "\n",
        "    gradient = torch.autograd.grad(loss, image)[0]\n",
        "\n",
        "    print(\"Gradient:\", gradient.dtype)\n",
        "\n",
        "    print(\"Gradient\", gradient.size)\n",
        "    sign_grad = torch.sign(gradient)\n",
        "\n",
        "    return sign_grad"
      ],
      "metadata": {
        "id": "YRULWTJGwu6b"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_idx = randint(0,49999)\n",
        "image = x_train[rand_idx].reshape((1, height, width, channels))\n",
        "label = y_train[rand_idx]\n",
        "print(label)\n",
        "\n",
        "labels = ['airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog',\n",
        "               'horse', 'ship', 'truck']\n",
        "\n",
        "matching_indices = np.where(label == 1)[0]\n",
        "if len(matching_indices) > 0:\n",
        "    predicted_label = labels[matching_indices[0]]\n",
        "    \n",
        "    print(f'Prediction from CNN: {predicted_label}')\n",
        "else:\n",
        "    print('No matching prediction found')\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(image.reshape((height, width, channels)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kyx-dyGnxBYJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "7e699eea-588c-4ecc-924e-298a78dc5afa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 1 0 0 0 0 0]\n",
            "Prediction from CNN: deer\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the adversary noise to image\n",
        "perturbations = generate_adversary(image,label).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "34y0UXJ6a4AJ",
        "outputId": "f0325c40-8612-47e2-cc57-8bb5ee70ad6c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-41e8377dd999>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Adding the adversary noise to image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mperturbations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_adversary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-89d6b2b5f0b9>\u001b[0m in \u001b[0;36mgenerate_adversary\u001b[0;34m(image, label)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#Image shape expected input[1, 32, 32, 3] to have 3 channels, but got 32 channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m#Prediction and label shapes need to match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predictions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mC:\\Users\\Ania\\Pytong\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mC:\\Users\\Ania\\PycharmProjects\\pythonProject5\\Resnets_20.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper_CUDA___slow_conv2d_forward)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert perturbations to a PyTorch tensor\n",
        "perturbations = torch.from_numpy(perturbations)\n",
        "perturbations = torch.transpose(perturbations, 1, 3)\n",
        "perturbations = perturbations * 0.1\n",
        "\n",
        "image = torch.from_numpy(image)\n",
        "\n",
        "print(type(image))  # <class 'torch.Tensor'>\n",
        "print(type(perturbations))  # <class 'torch.Tensor'>\n",
        "\n",
        "adversarial = image + perturbations\n",
        "\n",
        "print(adversarial.shape)\n",
        "print(image.shape)"
      ],
      "metadata": {
        "id": "dMyaxJd6zETs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing both images \n",
        "fig, (ax1,ax2) = plt.subplots(1, 2, sharey=True)\n",
        "ax1.imshow(image.reshape(height,width, channels))\n",
        "ax1.set_title(\"Original Image\")\n",
        "adversarial = adversarial.reshape(height,width, channels)\n",
        "ax2.imshow((adversarial))\n",
        "ax2.set_title(\"Image with Adversary\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lJfQp3n-zF4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the predictions\n",
        "image = image.reshape(1, channels, height, width)\n",
        "print(type(image))\n",
        "print(image.shape)\n",
        "normal_predictions = model(image.float()).argmax(dim=1)\n",
        "adversarial = adversarial.reshape(1, channels, height, width)\n",
        "adversary_predictions = model(adversarial.float()).argmax(dim=1)\n",
        "\n",
        "# Convert predictions to label names\n",
        "normal_label = label_names[normal_predictions.item()]\n",
        "adversary_label = label_names[adversary_predictions.item()]\n",
        "\n",
        "# Print the predictions\n",
        "print(f'Normal Image Prediction: {normal_label}')\n",
        "print(f\"Adversary Prediction: {adversary_label}\")"
      ],
      "metadata": {
        "id": "_uYApNFekl-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the images and their predicted labels\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
        "ax1.imshow(image.reshape(height,width, channels))\n",
        "ax1.set_title(f\"Original: {normal_label}\")\n",
        "ax2.imshow(adversarial.reshape(height,width, channels))\n",
        "ax2.set_title(f\"Adversarial: {adversary_label}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qa_O4MD0Ardx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate batch of images with adversary\n",
        "def adversary_generator(batch_size):\n",
        "  while True:\n",
        "    images = []\n",
        "    labels = []\n",
        "    for batch in range(batch_size):\n",
        "      N = randint(0, 49999)\n",
        "      label = y_train[N]\n",
        "      image = x_train[N].reshape((1,height, width, channels))\n",
        "\n",
        "      perturbations = generate_adversary(image, label).numpy()\n",
        "\n",
        "      print(image.shape, label.shape)\n",
        "      print(type(image))\n",
        "      image = np.swapaxes(image, 1, 3)\n",
        "\n",
        "      adversarial = image + (perturbations * 0.05)\n",
        "\n",
        "      images.append(adversarial)\n",
        "      labels.append(label)\n",
        "\n",
        "      if batch%1000 == 0:\n",
        "        print(f\"{batch} images generated\")\n",
        "\n",
        "    images = np.asarray(images).reshape((batch_size, height, width, channels))\n",
        "    labels = np.asarray(labels)\n",
        "\n",
        "    yield images, labels"
      ],
      "metadata": {
        "id": "3kKytyZ5zNwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "directory = '/content/drive/My Drive/CIFAR10/ATMC/'\n",
        "filename = 'adversarial_dataset.pth'\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(directory, exist_ok=True)"
      ],
      "metadata": {
        "id": "jZ-K1BW1pa03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_adversarial, y_adversarial = next(adversary_generator(10000))\n",
        "torch.save((x_adversarial, y_adversarial), os.path.join(directory, filename))"
      ],
      "metadata": {
        "id": "qTXBHPjwzWSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_filepath = os.path.join(directory, filename)\n",
        "adversarial_dataset = torch.load(model_filepath)"
      ],
      "metadata": {
        "id": "c_g7q6ie5zXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_adversarial, y_adversarial = adversarial_dataset\n",
        "import torch \n",
        "import numpy\n",
        "print(x_adversarial.shape)\n",
        "print(y_adversarial.shape)\n",
        "\n",
        "labels_names = ['airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog',\n",
        "               'horse', 'ship', 'truck']\n",
        "\n",
        "rand_idx = randint(0,9999)\n",
        "image = x_adversarial[rand_idx].reshape((1, height, width, channels))\n",
        "label = y_adversarial[rand_idx]\n",
        "\n",
        "matching_indices = np.where(label == 1)[0]\n",
        "if len(matching_indices) > 0:\n",
        "    predicted_label = label_names[matching_indices[0]]\n",
        "    print(f'Prediction from CNN: {predicted_label}')\n",
        "else:\n",
        "    print('No matching prediction found')\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(image.reshape((height, width, channels)))\n",
        "plt.show()\n",
        "# Calculate the accuracy\n",
        "\n",
        "batch_images = x_adversarial\n",
        "batch_labels = y_adversarial[1]\n",
        "\n",
        "batch_images = torch.from_numpy(batch_images)\n",
        "batch_images = torch.transpose(batch_images, 1, 3)\n",
        "batch_images = batch_images.to(torch.float32)\n",
        "\n",
        "\n",
        "# Forward pass to get the predictions\n",
        "batch_predictions = model(batch_images)\n",
        "\n",
        "# Calculate the number of correct predictions\n",
        "_, batch_predicted_labels = torch.max(batch_predictions, dim=1)\n",
        "correct_predictions += torch.sum(batch_predicted_labels == batch_labels).item()\n",
        "\n",
        "accuracy = correct_predictions / total_samples\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Accuracy: {accuracy:.2%}')"
      ],
      "metadata": {
        "id": "KvP4c-gSt6Ay"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyME4nruz1xgmZxTiLKEBVSB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}